{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AnotherMAMI.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OrionXV/Co-Lab-Misogny-Identifier/blob/main/AnotherMAMI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multimedia Automatic Misogyny Identification (MAMI)... <i> 3: The New Me </i>\n",
        "\n",
        "A personal endeavour by Syed Arsalaan Nadim for the timebeing. \n",
        "In order to avoid clutter I am making a personal notebook.\n",
        "\n"
      ],
      "metadata": {
        "id": "5uSTVngHoTMO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTe5O9nNoIs8",
        "outputId": "1e0fd92f-cfa9-4625-eb1a-28d6fbcb6239"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install pytorch_lightning \n",
        "!pip install fasttext"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns  \n",
        "sns.set()\n",
        "from matplotlib import pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "%config InLineBackend.figure_format = 'retina'\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import csv\n",
        "import json\n",
        "import logging\n",
        "from pathlib import Path\n",
        "import random\n",
        "import tarfile\n",
        "import tempfile\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch                    \n",
        "import torchvision\n",
        "import fasttext"
      ],
      "metadata": {
        "id": "oCtpibnpokM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1lqpl_ofWT1aJAG-aHFbgFdKpWx01QDv7' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1lqpl_ofWT1aJAG-aHFbgFdKpWx01QDv7\" -O train.zip && rm -rf /tmp/cookies.txt"
      ],
      "metadata": {
        "id": "dMhmWAhYoqDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=12KwkIoljStd8diaw5Aqz_RsKityaFudr' -O trial.zip"
      ],
      "metadata": {
        "id": "1DH3tBeNosdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q train.zip"
      ],
      "metadata": {
        "id": "mxkh9rmsouBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q -P *MaMiSemEval2022! trial.zip"
      ],
      "metadata": {
        "id": "_sqlLgpWovnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "path = './TRAINING'\n",
        "extension = 'csv'\n",
        "os.chdir(path)\n",
        "result = glob.glob('*.{}'.format(extension))\n",
        "print(result)"
      ],
      "metadata": {
        "id": "pVADCXozoxxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = Path.cwd().parent\n",
        "\n",
        "training_path = data_path / \"TRAINING\"\n",
        "train_csv_path = training_path / \"training.csv\"\n",
        "trial_path = data_path / \"Users\" / \"fersiniel\" / \"Desktop/MAMI - TO LABEL\" / \"TRIAL DATASET\" \n",
        "trial_csv_path = trial_path / \"trial.csv\"\n",
        "data_dir = training_path"
      ],
      "metadata": {
        "id": "qTiHwDN5oygv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " data = pd.read_csv(train_csv_path, usecols=['file_name', 'misogynous', 'Text Transcription'], sep='\\t')"
      ],
      "metadata": {
        "id": "_6NY9I6No4Uf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(), data['misogynous'].value_counts(), data['Text Transcription'].map(lambda text: len(text.split(\" \"))).describe()"
      ],
      "metadata": {
        "id": "1hUG5NJ3o89o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "\n",
        "images = [\n",
        "    Image.open(\n",
        "        training_path / data.loc[i, \"file_name\"]\n",
        "    ).convert(\"RGB\")\n",
        "    for i in range(16)\n",
        "]\n",
        "\n",
        "for image in images:\n",
        "    print(image.size)"
      ],
      "metadata": {
        "id": "ZSRaTGyEpCBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_transform = torchvision.transforms.Compose(\n",
        "    [\n",
        "        torchvision.transforms.Resize(size=(200, 200)),\n",
        "        torchvision.transforms.ToTensor()\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "AhWd54ONpGDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_img = torch.stack(\n",
        "    [image_transform(image) for image in images]\n",
        ")\n",
        "grid = torchvision.utils.make_grid(tensor_img)\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
        "plt.axis('off')\n",
        "plt.imshow(grid.permute(1, 2, 0))"
      ],
      "metadata": {
        "id": "SSK3QcpypIRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Creation "
      ],
      "metadata": {
        "id": "ZlsRmNmsq_hQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Set Creation "
      ],
      "metadata": {
        "id": "U8r-IWpgrGCa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MemeDataset(torch.utils.data.Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_path,\n",
        "        img_dir,\n",
        "        image_transform,\n",
        "        text_transform\n",
        "        \n",
        "    ):\n",
        "        self.samples_frame = pd.read_csv(data_path, sep='\\t')\n",
        "\n",
        "        self.samples_frame['file_path'] = self.samples_frame['file_name'].apply(\n",
        "            lambda row: (Path(str(img_dir) + \"/\" +str(row)))\n",
        "            ) \n",
        "\n",
        "        #self.samples_frame.set_index('file_name')\n",
        "\n",
        "        self.img_dir = img_dir\n",
        "        self.image_transform = image_transform\n",
        "        self.text_transform = text_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples_frame)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"This method is called when you do instance[key] \n",
        "        for an instance of this class.\n",
        "        \"\"\"\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        img_id = torch.Tensor(\n",
        "            self.text_transform.get_sentence_vector(\n",
        "                self.samples_frame.loc[idx, 'file_name'].replace(\"\\n\", \"\") #.apply(lambda id: id.strip()).to_string().\n",
        "            )\n",
        "        ).squeeze()\n",
        "       \n",
        "        image = Image.open(\n",
        "            self.samples_frame.loc[idx, \"file_path\"]#.to_string()\n",
        "        ).convert(\"RGB\")\n",
        "\n",
        "        image = self.image_transform(image)\n",
        "\n",
        "        text = torch.Tensor(\n",
        "            self.text_transform.get_sentence_vector(\n",
        "                self.samples_frame.loc[idx, \"Text Transcription\"].replace(\"\\n\", \"\") #.apply(lambda id: id.strip()).to_string().\n",
        "            )\n",
        "        ).squeeze()\n",
        "\n",
        "        if \"misogynous\" in self.samples_frame.columns:\n",
        "            label = torch.Tensor(\n",
        "                [self.samples_frame.loc[idx, \"misogynous\"]]\n",
        "            ).long().squeeze()\n",
        "            sample = {\n",
        "                \"file_name\": img_id, \n",
        "                \"image\": image, \n",
        "                \"Text Transcription\": text, \n",
        "                \"misogynous\": label\n",
        "            }\n",
        "        else:\n",
        "            sample = {\n",
        "                \"file_name\": img_id, \n",
        "                \"image\": image, \n",
        "                \"Text Transcription\": text\n",
        "            }\n",
        "\n",
        "        return sample"
      ],
      "metadata": {
        "id": "ZCXiwYvNpPKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LangAndVisionConcat(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes,\n",
        "        loss_fn,\n",
        "        language_module,\n",
        "        vision_module,\n",
        "        language_feature_dim,\n",
        "        vision_feature_dim,\n",
        "        fusion_output_size,\n",
        "        dropout_p,     \n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.language_module = language_module\n",
        "        self.vision_module = vision_module\n",
        "\n",
        "        self.fusion = torch.nn.Linear(\n",
        "            in_features=(language_feature_dim + vision_feature_dim), \n",
        "            out_features=fusion_output_size\n",
        "        )\n",
        "\n",
        "        self.fc = torch.nn.Linear(\n",
        "            in_features=fusion_output_size, \n",
        "            out_features=num_classes\n",
        "        )\n",
        "\n",
        "        self.loss_fn = loss_fn\n",
        "        self.dropout = torch.nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, text, image, label = None):\n",
        "        text_features = torch.nn.functional.relu(self.language_module(text))\n",
        "        image_features = torch.nn.functional.relu(self.vision_module(image))\n",
        "        combined = torch.cat([text_features, image_features], dim=1)\n",
        "        fused = self.dropout(torch.nn.functional.relu(self.fusion(combined)))\n",
        "        logits = self.fc(fused)\n",
        "        pred = torch.nn.functional.softmax(logits)\n",
        "        loss = (self.loss_fn(pred, label) if label is not None else label)\n",
        "        return (pred, loss)"
      ],
      "metadata": {
        "id": "N9Yt8sHT5-f4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics"
      ],
      "metadata": {
        "id": "s58EglnG6pE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pytorch_lightning as pl\n",
        "from torchmetrics import functional as FM \n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "logging.getLogger().setLevel(logging.WARNING)\n",
        "\n",
        "\n",
        "class MemesModel(pl.LightningModule):\n",
        "    def __init__(self, hparams):\n",
        "        for data_key in [\n",
        "                         \"train_path\", \n",
        "                         \"trial_path\", \n",
        "                         \"train_img_dir\",\n",
        "                         \"trial_img_dir\"\n",
        "                    ]:\n",
        "            if data_key not in hparams.keys():\n",
        "                raise KeyError(\n",
        "                    f\"{data_key} is a required hparam in this model\"\n",
        "                )\n",
        "        \n",
        "        super().__init__()\n",
        "        self._hparams = hparams \n",
        "\n",
        "        # assign some hparams that get used in multiple places\n",
        "        self.embedding_dim = self.hparams.get(\"embedding_dim\", 300)\n",
        "        self.language_feature_dim = self.hparams.get(\"language_feature_dim\", 300)\n",
        "\n",
        "        # balance language and vision features by default\n",
        "        self.vision_feature_dim = self.hparams.get(\"vision_feature_dim\", self.language_feature_dim)\n",
        "        self.output_path = Path(self.hparams.get(\"output_path\", \"model-outputs\"))\n",
        "        self.output_path.mkdir(exist_ok=True)\n",
        "        \n",
        "        # instantiate transforms, datasets\n",
        "        self.text_transform = self._build_text_transform()\n",
        "        self.image_transform = self._build_image_transform()\n",
        "        self.train_dataset = self._build_dataset(data_key=\"train_path\", img_dir=\"train_img_dir\")\n",
        "        self.trial_dataset = self._build_dataset(data_key=\"trial_path\", img_dir=\"test_img_dir\")\n",
        "        \n",
        "        # set up model and training\n",
        "        self.model = self._build_model()\n",
        "        self.trainer_params = self._get_trainer_params()\n",
        "    \n",
        "    @property\n",
        "    def hparams(self):\n",
        "        return self._hparams\n",
        "\n",
        "    # Required LightningModule Methods (when validating) \n",
        "    \n",
        "    def forward(self, text, image, label=None):\n",
        "        return self.model(text, image, label)\n",
        "\n",
        "    def training_step(self, batch, batch_nb):\n",
        "        preds, loss = self.forward(\n",
        "            text=batch[\"Text Transcription\"], \n",
        "            image=batch[\"image\"], \n",
        "            label=batch[\"misogynous\"]\n",
        "        )\n",
        "        \n",
        "        return {\"loss\": loss}\n",
        "\n",
        "    def validation_step(self, batch, batch_nb):\n",
        "        preds, loss = self.eval().forward(\n",
        "            text=batch[\"Text Transcription\"], \n",
        "            image=batch[\"image\"], \n",
        "            label=batch[\"misogynous\"]\n",
        "        )\n",
        "\n",
        "        acc = FM.accuracy(preds, batch[\"misogynous\"], num_classes=2)\n",
        "        self.log(\"val_loss\", loss)\n",
        "        \n",
        "        return {\"batch_val_loss\": loss, \"batch_val_acc\": acc}\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        avg_loss = torch.stack(\n",
        "            tuple(\n",
        "                output[\"batch_val_loss\"] \n",
        "                for output in outputs\n",
        "            )\n",
        "        ).mean()\n",
        "        \n",
        "        avg_acc = torch.stack(\n",
        "            tuple(\n",
        "                output[\"batch_val_acc\"] \n",
        "                for output in outputs\n",
        "            )\n",
        "        ).mean()\n",
        "\n",
        "        return {\n",
        "            \"val_loss\": avg_loss,\n",
        "            \"progress_bar\":{\n",
        "                \"avg_val_loss\": avg_loss, \n",
        "                \"avg_val_acc\": avg_acc\n",
        "                }\n",
        "        }\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizers = [\n",
        "            torch.optim.AdamW(\n",
        "                self.model.parameters(), \n",
        "                lr=self.hparams.get(\"lr\", 0.001)\n",
        "            )\n",
        "        ]\n",
        "        schedulers = [{'scheduler': x, 'monitor': 'val_loss'} for x in [\n",
        "            torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "                optimizers[0]\n",
        "            )\n",
        "        ]]\n",
        "        tu = tuple(\n",
        "            [\n",
        "             {'optimizer' : op, 'lr_scheduler' : sch} for op, sch in zip(optimizers, schedulers)\n",
        "            ]\n",
        "        )\n",
        "        return tu\n",
        "    \n",
        "    #@pl.data_loader\n",
        "    def train_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            self.train_dataset, \n",
        "            shuffle=True, \n",
        "            batch_size=self.hparams.get(\"batch_size\", 4), \n",
        "            num_workers=self.hparams.get(\"num_workers\", 16)\n",
        "        )\n",
        "\n",
        "    #@pl.data_loader\n",
        "    def val_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            #self.trial_dataset,\n",
        "            self.train_dataset, \n",
        "            shuffle=False, \n",
        "            batch_size=self.hparams.get(\"batch_size\", 4), \n",
        "            num_workers=self.hparams.get(\"num_workers\", 16),\n",
        "        )\n",
        "\n",
        "    def fit(self):\n",
        "        self.trainer = pl.Trainer(**self.trainer_params)\n",
        "        self.trainer.fit(self)\n",
        "    \n",
        "    # will improve further on this by making a few more methods... or well specifying them \n",
        "    \n",
        "    def _build_text_transform(self):\n",
        "        with tempfile.NamedTemporaryFile() as ft_training_data:\n",
        "            ft_path = Path(ft_training_data.name)\n",
        "            with ft_path.open(\"w\") as ft:\n",
        "\n",
        "                training_data = [ \n",
        "                                 str(line) + \"/n\" \n",
        "                                 for line in pd.read_csv(self.hparams.get('train_path'), usecols = [\"Text Transcription\"], sep = '\\t')['Text Transcription']\n",
        "                ]\n",
        "\n",
        "                for line in training_data:\n",
        "                    ft.write(line + \"\\n\")\n",
        "                language_transform = fasttext.train_unsupervised(\n",
        "                    str(ft_path),\n",
        "                    model=self.hparams.get(\"fasttext_model\", \"cbow\"),\n",
        "                    dim=self.embedding_dim\n",
        "                )\n",
        "        return language_transform\n",
        "    \n",
        "    def _build_image_transform(self):\n",
        "        image_dim = self.hparams.get(\"image_dim\", 200)\n",
        "        image_transform = torchvision.transforms.Compose(\n",
        "            [\n",
        "                torchvision.transforms.Resize(\n",
        "                    size=(image_dim, image_dim)\n",
        "                ),        \n",
        "                torchvision.transforms.ToTensor(),\n",
        "                # all torchvision models expect the same\n",
        "                # normalization mean and std\n",
        "                # https://pytorch.org/docs/stable/torchvision/models.html\n",
        "                torchvision.transforms.Normalize(\n",
        "                    mean=(0.485, 0.456, 0.406), \n",
        "                    std=(0.229, 0.224, 0.225)\n",
        "                ),\n",
        "            ]\n",
        "        )\n",
        "        return image_transform\n",
        "\n",
        "    def _build_dataset(self, data_key, img_dir):\n",
        "        return MemeDataset(\n",
        "            data_path = self.hparams.get(data_key),\n",
        "            img_dir = self.hparams.get(img_dir), \n",
        "            image_transform = self.image_transform,\n",
        "            text_transform = self.text_transform,\n",
        "            # limit training samples only\n",
        "        )\n",
        "\n",
        "    def _build_model(self):\n",
        "        # we're going to pass the outputs of our text\n",
        "        # transform through an additional trainable layer\n",
        "        # rather than fine-tuning the transform\n",
        "        language_module = torch.nn.Linear(\n",
        "                in_features=self.embedding_dim,\n",
        "                out_features=self.language_feature_dim\n",
        "        )\n",
        "        \n",
        "        # easiest way to get features rather than\n",
        "        # classification is to overwrite last layer\n",
        "        # with an identity transformation, we'll reduce\n",
        "        # dimension using a Linear layer, resnet is 2048 out\n",
        "        vision_module = torchvision.models.resnet152(\n",
        "            pretrained=True\n",
        "        )\n",
        "        vision_module.fc = torch.nn.Linear(\n",
        "                in_features=2048,\n",
        "                out_features=self.vision_feature_dim\n",
        "        )\n",
        "\n",
        "        return LangAndVisionConcat(\n",
        "            num_classes = self.hparams.get(\"num_classes\", 2),\n",
        "            loss_fn = torch.nn.CrossEntropyLoss(),\n",
        "            language_module = language_module,\n",
        "            vision_module = vision_module,\n",
        "            language_feature_dim = self.language_feature_dim,\n",
        "            vision_feature_dim = self.vision_feature_dim,\n",
        "            fusion_output_size = self.hparams.get(\n",
        "                \"fusion_output_size\", 512\n",
        "            ),\n",
        "            dropout_p = self.hparams.get(\"dropout_p\", 0.1),\n",
        "        )\n",
        "\n",
        "    def _get_trainer_params(self):\n",
        "        checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
        "            dirpath = self.output_path,\n",
        "            monitor = self.hparams.get(\n",
        "                \"checkpoint_monitor\", \"val_loss\"\n",
        "            ),\n",
        "            mode = self.hparams.get(\n",
        "                \"checkpoint_monitor_mode\", \"max\"\n",
        "            ),\n",
        "            verbose = self.hparams.get(\"verbose\", True)\n",
        "        )\n",
        "\n",
        "        early_stop_callback = pl.callbacks.EarlyStopping(\n",
        "            monitor = self.hparams.get(\n",
        "                \"early_stop_monitor\", \"val_loss\"\n",
        "            ),\n",
        "            min_delta = self.hparams.get(\n",
        "                \"early_stop_min_delta\", 0.001\n",
        "            ),\n",
        "            patience = self.hparams.get(\n",
        "                \"early_stop_patience\", 5\n",
        "            ),\n",
        "            verbose = self.hparams.get(\"verbose\", True),\n",
        "        )\n",
        "\n",
        "        trainer_params = {\n",
        "            \"callbacks\" : [early_stop_callback],\n",
        "            \"checkpoint_callback\": checkpoint_callback,\n",
        "            #\"early_stop_callback\": early_stop_callback,\n",
        "            #\"default_save_path\": self.output_path,\n",
        "            \"accumulate_grad_batches\": self.hparams.get(\n",
        "                \"accumulate_grad_batches\", 1\n",
        "            ),\n",
        "            \"gpus\": self.hparams.get(\"n_gpu\", 1),\n",
        "            \"max_epochs\": self.hparams.get(\"max_epochs\", 100),\n",
        "            \"gradient_clip_val\": self.hparams.get(\n",
        "                \"gradient_clip_value\", 1\n",
        "            ),\n",
        "        }\n",
        "        return trainer_params\n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def make_submission_frame(self, train_path):\n",
        "        train_dataset = self._build_dataset(train_path)\n",
        "        submission_frame = pd.DataFrame(\n",
        "            index = train_dataset.samples_frame.file_name,\n",
        "            columns=[\"proba\", \"misogynous\"]\n",
        "        )\n",
        "\n",
        "        train_dataloader = torch.utils.data.DataLoader(\n",
        "            train_dataset, \n",
        "            shuffle = False, \n",
        "            batch_size = self.hparams.get(\"batch_size\", 4), \n",
        "            num_workers = self.hparams.get(\"num_workers\", 16))\n",
        "        for batch in tqdm(train_dataloader, total = len(train_dataloader)):\n",
        "            preds, _ = self.model.eval().to(\"cpu\")(\n",
        "                batch[\"Text Transcription\"], batch[\"image\"]\n",
        "            )\n",
        "            submission_frame.loc[batch[\"file_name\"], \"proba\"] = preds[:, 1]\n",
        "            submission_frame.loc[batch[\"file_name\"], \"misogynous\"] = preds.argmax(dim=1)\n",
        "        submission_frame.proba = submission_frame.proba.astype(float)\n",
        "        submission_frame.label = submission_frame.misogynous.astype(int)\n",
        "        return submission_frame    \n",
        "    "
      ],
      "metadata": {
        "id": "y1-uZ-UH602G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hparams = {\n",
        "    \"train_path\": train_csv_path,\n",
        "    \"trial_path\": trial_csv_path,\n",
        "    \"train_img_dir\": data_dir,\n",
        "    \"trial_img_dir\": trial_path,\n",
        "\n",
        "    \n",
        "\n",
        "    \"embedding_dim\": 150,\n",
        "    \"language_feature_dim\": 300,\n",
        "    \"vision_feature_dim\": 300,\n",
        "    \"fusion_output_size\": 256,\n",
        "    \"output_path\": \"model-outputs\",\n",
        "    \"lr\": 0.005,\n",
        "    \"max_epochs\": 10,\n",
        "    \"n_gpu\": 1,\n",
        "    \"batch_size\": 8,\n",
        "    # allows us to \"simulate\" having larger batches \n",
        "    \"accumulate_grad_batches\": 16,\n",
        "    \"early_stop_patience\": 3,\n",
        "}\n",
        "\n",
        "\n",
        "miso_memes_model = MemesModel(hparams = hparams)\n",
        "miso_memes_model.fit()"
      ],
      "metadata": {
        "id": "5cuLS0aQWRS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YNYk4lPsYAQ8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
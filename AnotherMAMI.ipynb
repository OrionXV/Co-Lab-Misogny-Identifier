{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OrionXV/Co-Lab-Misogny-Identifier/blob/main/AnotherMAMI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uSTVngHoTMO"
      },
      "source": [
        "# Multimedia Automatic Misogyny Identification (MAMI)... <i> 3: The New Me </i>\n",
        "\n",
        "A personal endeavour by Syed Arsalaan Nadim for the timebeing. \n",
        "In order to avoid clutter I am making a personal notebook.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTe5O9nNoIs8",
        "outputId": "a90a681d-fdb4-479d-aadd-3f73c3e26f11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n",
            "Collecting pytorch_lightning\n",
            "  Downloading pytorch_lightning-1.5.8-py3-none-any.whl (526 kB)\n",
            "\u001b[K     |████████████████████████████████| 526 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.19.5)\n",
            "Collecting pyDeprecate==0.3.1\n",
            "  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n",
            "Collecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 44.5 MB/s \n",
            "\u001b[?25hCollecting PyYAML>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 55.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (3.10.0.2)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (2.7.0)\n",
            "Collecting torchmetrics>=0.4.1\n",
            "  Downloading torchmetrics-0.6.2-py3-none-any.whl (332 kB)\n",
            "\u001b[K     |████████████████████████████████| 332 kB 52.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (4.62.3)\n",
            "Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
            "  Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 53.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.7.* in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.10.0+cu111)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.23.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 42.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch_lightning) (3.0.6)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.4.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (57.4.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.17.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.3.6)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.37.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.42.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.12.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.8.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch_lightning) (1.15.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (3.1.1)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.0.9)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 51.4 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 48.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (21.2.0)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB)\n",
            "\u001b[K     |████████████████████████████████| 192 kB 65.1 MB/s \n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Building wheels for collected packages: future\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=cce3e3389e3941f3f1375cbc6260deec8d7f30a6f5817bb4525356b9966d7f17\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "Successfully built future\n",
            "Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, fsspec, aiohttp, torchmetrics, PyYAML, pyDeprecate, future, pytorch-lightning\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "Successfully installed PyYAML-6.0 aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 frozenlist-1.2.0 fsspec-2021.11.1 future-0.18.2 multidict-5.2.0 pyDeprecate-0.3.1 pytorch-lightning-1.5.8 torchmetrics-0.6.2 yarl-1.7.2\n",
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 3.3 MB/s \n",
            "\u001b[?25hCollecting pybind11>=2.2\n",
            "  Using cached pybind11-2.9.0-py2.py3-none-any.whl (210 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext) (57.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext) (1.19.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3125926 sha256=4e5f5fd3f12488088fb94417788db486194faa285a58ff74b1feb5f97758c361\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/ca/bf/b020d2be95f7641801a6597a29c8f4f19e38f9c02a345bab9b\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install pytorch_lightning \n",
        "!pip install fasttext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oCtpibnpokM7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns  \n",
        "sns.set()\n",
        "from matplotlib import pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "%config InLineBackend.figure_format = 'retina'\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import csv\n",
        "import json\n",
        "import logging\n",
        "from pathlib import Path\n",
        "import random\n",
        "import tarfile\n",
        "import tempfile\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch                    \n",
        "import torchvision\n",
        "import fasttext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMhmWAhYoqDB"
      },
      "outputs": [],
      "source": [
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1lqpl_ofWT1aJAG-aHFbgFdKpWx01QDv7' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1lqpl_ofWT1aJAG-aHFbgFdKpWx01QDv7\" -O train.zip && rm -rf /tmp/cookies.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DH3tBeNosdX"
      },
      "outputs": [],
      "source": [
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=12KwkIoljStd8diaw5Aqz_RsKityaFudr' -O trial.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxkh9rmsouBv"
      },
      "outputs": [],
      "source": [
        "!unzip -q train.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sqlLgpWovnn"
      },
      "outputs": [],
      "source": [
        "!unzip -q -P *MaMiSemEval2022! trial.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVADCXozoxxw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "path = './TRAINING'\n",
        "extension = 'csv'\n",
        "os.chdir(path)\n",
        "result = glob.glob('*.{}'.format(extension))\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTiHwDN5oygv"
      },
      "outputs": [],
      "source": [
        "data_path = Path.cwd().parent\n",
        "\n",
        "training_path = data_path / \"TRAINING\"\n",
        "train_csv_path = training_path / \"training.csv\"\n",
        "trial_path = data_path / \"Users\" / \"fersiniel\" / \"Desktop/MAMI - TO LABEL\" / \"TRIAL DATASET\" \n",
        "trial_csv_path = trial_path / \"trial.csv\"\n",
        "data_dir = training_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6NY9I6No4Uf"
      },
      "outputs": [],
      "source": [
        " data = pd.read_csv(train_csv_path, usecols=['file_name', 'misogynous', 'Text Transcription'], sep='\\t')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hUG5NJ3o89o"
      },
      "outputs": [],
      "source": [
        "data.head(), data['misogynous'].value_counts(), data['Text Transcription'].map(lambda text: len(text.split(\" \"))).describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSRaTGyEpCBf"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "\n",
        "images = [\n",
        "    Image.open(\n",
        "        training_path / data.loc[i, \"file_name\"]\n",
        "    ).convert(\"RGB\")\n",
        "    for i in range(16)\n",
        "]\n",
        "\n",
        "for image in images:\n",
        "    print(image.size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhWd54ONpGDw"
      },
      "outputs": [],
      "source": [
        "image_transform = torchvision.transforms.Compose(\n",
        "    [\n",
        "        torchvision.transforms.Resize(size=(200, 200)),\n",
        "        torchvision.transforms.ToTensor()\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSK3QcpypIRp"
      },
      "outputs": [],
      "source": [
        "tensor_img = torch.stack(\n",
        "    [image_transform(image) for image in images]\n",
        ")\n",
        "grid = torchvision.utils.make_grid(tensor_img)\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
        "plt.axis('off')\n",
        "plt.imshow(grid.permute(1, 2, 0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlsRmNmsq_hQ"
      },
      "source": [
        "# Model Creation "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8r-IWpgrGCa"
      },
      "source": [
        "Data Set Creation "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCXiwYvNpPKp"
      },
      "outputs": [],
      "source": [
        "class MemeDataset(torch.utils.data.Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_path,\n",
        "        img_dir,\n",
        "        image_transform,\n",
        "        text_transform\n",
        "        \n",
        "    ):\n",
        "        self.samples_frame = pd.read_csv(data_path, sep='\\t')\n",
        "\n",
        "        self.samples_frame['file_path'] = self.samples_frame['file_name'].apply(\n",
        "            lambda row: (Path(str(img_dir) + \"/\" +str(row)))\n",
        "            ) \n",
        "\n",
        "        #self.samples_frame.set_index('file_name')\n",
        "\n",
        "        self.img_dir = img_dir\n",
        "        self.image_transform = image_transform\n",
        "        self.text_transform = text_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples_frame)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"This method is called when you do instance[key] \n",
        "        for an instance of this class.\n",
        "        \"\"\"\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        img_id = torch.Tensor(\n",
        "            self.text_transform.get_sentence_vector(\n",
        "                self.samples_frame.loc[idx, 'file_name'].replace(\"\\n\", \"\") #.apply(lambda id: id.strip()).to_string().\n",
        "            )\n",
        "        ).squeeze()\n",
        "       \n",
        "        image = Image.open(\n",
        "            self.samples_frame.loc[idx, \"file_path\"]#.to_string()\n",
        "        ).convert(\"RGB\")\n",
        "\n",
        "        image = self.image_transform(image)\n",
        "\n",
        "        text = torch.Tensor(\n",
        "            self.text_transform.get_sentence_vector(\n",
        "                self.samples_frame.loc[idx, \"Text Transcription\"].replace(\"\\n\", \"\") #.apply(lambda id: id.strip()).to_string().\n",
        "            )\n",
        "        ).squeeze()\n",
        "\n",
        "        if \"misogynous\" in self.samples_frame.columns:\n",
        "            label = torch.Tensor(\n",
        "                [self.samples_frame.loc[idx, \"misogynous\"]]\n",
        "            ).long().squeeze()\n",
        "            sample = {\n",
        "                \"file_name\": img_id, \n",
        "                \"image\": image, \n",
        "                \"Text Transcription\": text, \n",
        "                \"misogynous\": label\n",
        "            }\n",
        "        else:\n",
        "            sample = {\n",
        "                \"file_name\": img_id, \n",
        "                \"image\": image, \n",
        "                \"Text Transcription\": text\n",
        "            }\n",
        "\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9Yt8sHT5-f4"
      },
      "outputs": [],
      "source": [
        "class LangAndVisionConcat(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes,\n",
        "        loss_fn,\n",
        "        language_module,\n",
        "        vision_module,\n",
        "        language_feature_dim,\n",
        "        vision_feature_dim,\n",
        "        fusion_output_size,\n",
        "        dropout_p,     \n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.language_module = language_module\n",
        "        self.vision_module = vision_module\n",
        "\n",
        "        self.fusion = torch.nn.Linear(\n",
        "            in_features=(language_feature_dim + vision_feature_dim), \n",
        "            out_features=fusion_output_size\n",
        "        )\n",
        "\n",
        "        self.fc = torch.nn.Linear(\n",
        "            in_features=fusion_output_size, \n",
        "            out_features=num_classes\n",
        "        )\n",
        "\n",
        "        self.loss_fn = loss_fn\n",
        "        self.dropout = torch.nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, text, image, label = None):\n",
        "        text_features = torch.nn.functional.relu(self.language_module(text))\n",
        "        image_features = torch.nn.functional.relu(self.vision_module(image))\n",
        "        combined = torch.cat([text_features, image_features], dim=1)\n",
        "        fused = self.dropout(torch.nn.functional.relu(self.fusion(combined)))\n",
        "        logits = self.fc(fused)\n",
        "        pred = torch.nn.functional.softmax(logits)\n",
        "        loss = (self.loss_fn(pred, label) if label is not None else label)\n",
        "        return (pred, loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s58EglnG6pE_"
      },
      "outputs": [],
      "source": [
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1-uZ-UH602G"
      },
      "outputs": [],
      "source": [
        "import pytorch_lightning as pl\n",
        "from torchmetrics import functional as FM \n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "logging.getLogger().setLevel(logging.WARNING)\n",
        "\n",
        "\n",
        "class MemesModel(pl.LightningModule):\n",
        "    def __init__(self, hparams):\n",
        "        for data_key in [\n",
        "                         \"train_path\", \n",
        "                         \"trial_path\", \n",
        "                         \"train_img_dir\",\n",
        "                         \"trial_img_dir\"\n",
        "                    ]:\n",
        "            if data_key not in hparams.keys():\n",
        "                raise KeyError(\n",
        "                    f\"{data_key} is a required hparam in this model\"\n",
        "                )\n",
        "        \n",
        "        super().__init__()\n",
        "        self._hparams = hparams \n",
        "\n",
        "        # assign some hparams that get used in multiple places\n",
        "        self.embedding_dim = self.hparams.get(\"embedding_dim\", 300)\n",
        "        self.language_feature_dim = self.hparams.get(\"language_feature_dim\", 300)\n",
        "\n",
        "        # balance language and vision features by default\n",
        "        self.vision_feature_dim = self.hparams.get(\"vision_feature_dim\", self.language_feature_dim)\n",
        "        self.output_path = Path(self.hparams.get(\"output_path\", \"model-outputs\"))\n",
        "        self.output_path.mkdir(exist_ok=True)\n",
        "        \n",
        "        # instantiate transforms, datasets\n",
        "        self.text_transform = self._build_text_transform()\n",
        "        self.image_transform = self._build_image_transform()\n",
        "        self.train_dataset = self._build_dataset(data_key=\"train_path\", img_dir=\"train_img_dir\")\n",
        "        self.trial_dataset = self._build_dataset(data_key=\"trial_path\", img_dir=\"test_img_dir\")\n",
        "        \n",
        "        # set up model and training\n",
        "        self.model = self._build_model()\n",
        "        self.trainer_params = self._get_trainer_params()\n",
        "    \n",
        "    @property\n",
        "    def hparams(self):\n",
        "        return self._hparams\n",
        "\n",
        "    # Required LightningModule Methods (when validating) \n",
        "    \n",
        "    def forward(self, text, image, label=None):\n",
        "        return self.model(text, image, label)\n",
        "\n",
        "    def training_step(self, batch, batch_nb):\n",
        "        preds, loss = self.forward(\n",
        "            text=batch[\"Text Transcription\"], \n",
        "            image=batch[\"image\"], \n",
        "            label=batch[\"misogynous\"]\n",
        "        )\n",
        "        \n",
        "        return {\"loss\": loss}\n",
        "\n",
        "    def validation_step(self, batch, batch_nb):\n",
        "        preds, loss = self.eval().forward(\n",
        "            text=batch[\"Text Transcription\"], \n",
        "            image=batch[\"image\"], \n",
        "            label=batch[\"misogynous\"]\n",
        "        )\n",
        "\n",
        "        acc = FM.accuracy(preds, batch[\"misogynous\"], num_classes=2)\n",
        "        self.log(\"val_loss\", loss)\n",
        "        \n",
        "        return {\"batch_val_loss\": loss, \"batch_val_acc\": acc}\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        avg_loss = torch.stack(\n",
        "            tuple(\n",
        "                output[\"batch_val_loss\"] \n",
        "                for output in outputs\n",
        "            )\n",
        "        ).mean()\n",
        "        \n",
        "        avg_acc = torch.stack(\n",
        "            tuple(\n",
        "                output[\"batch_val_acc\"] \n",
        "                for output in outputs\n",
        "            )\n",
        "        ).mean()\n",
        "\n",
        "        return {\n",
        "            \"val_loss\": avg_loss,\n",
        "            \"progress_bar\":{\n",
        "                \"avg_val_loss\": avg_loss, \n",
        "                \"avg_val_acc\": avg_acc\n",
        "                }\n",
        "        }\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizers = [\n",
        "            torch.optim.AdamW(\n",
        "                self.model.parameters(), \n",
        "                lr=self.hparams.get(\"lr\", 0.001)\n",
        "            )\n",
        "        ]\n",
        "        schedulers = [{'scheduler': x, 'monitor': 'val_loss'} for x in [\n",
        "            torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "                optimizers[0]\n",
        "            )\n",
        "        ]]\n",
        "        tu = tuple(\n",
        "            [\n",
        "             {'optimizer' : op, 'lr_scheduler' : sch} for op, sch in zip(optimizers, schedulers)\n",
        "            ]\n",
        "        )\n",
        "        return tu\n",
        "    \n",
        "    #@pl.data_loader\n",
        "    def train_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            self.train_dataset, \n",
        "            shuffle=True, \n",
        "            batch_size=self.hparams.get(\"batch_size\", 4), \n",
        "            num_workers=self.hparams.get(\"num_workers\", 16)\n",
        "        )\n",
        "\n",
        "    #@pl.data_loader\n",
        "    def val_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            #self.trial_dataset,\n",
        "            self.train_dataset, \n",
        "            shuffle=False, \n",
        "            batch_size=self.hparams.get(\"batch_size\", 4), \n",
        "            num_workers=self.hparams.get(\"num_workers\", 16),\n",
        "        )\n",
        "\n",
        "    def fit(self):\n",
        "        self.trainer = pl.Trainer(**self.trainer_params)\n",
        "        self.trainer.fit(self)\n",
        "    \n",
        "    # will improve further on this by making a few more methods... or well specifying them \n",
        "    \n",
        "    def _build_text_transform(self):\n",
        "        with tempfile.NamedTemporaryFile() as ft_training_data:\n",
        "            ft_path = Path(ft_training_data.name)\n",
        "            with ft_path.open(\"w\") as ft:\n",
        "\n",
        "                training_data = [ \n",
        "                                 str(line) + \"/n\" \n",
        "                                 for line in pd.read_csv(self.hparams.get('train_path'), usecols = [\"Text Transcription\"], sep = '\\t')['Text Transcription']\n",
        "                ]\n",
        "\n",
        "                for line in training_data:\n",
        "                    ft.write(line + \"\\n\")\n",
        "                language_transform = fasttext.train_unsupervised(\n",
        "                    str(ft_path),\n",
        "                    model=self.hparams.get(\"fasttext_model\", \"cbow\"),\n",
        "                    dim=self.embedding_dim\n",
        "                )\n",
        "        return language_transform\n",
        "    \n",
        "    def _build_image_transform(self):\n",
        "        image_dim = self.hparams.get(\"image_dim\", 200)\n",
        "        image_transform = torchvision.transforms.Compose(\n",
        "            [\n",
        "                torchvision.transforms.Resize(\n",
        "                    size=(image_dim, image_dim)\n",
        "                ),        \n",
        "                torchvision.transforms.ToTensor(),\n",
        "                # all torchvision models expect the same\n",
        "                # normalization mean and std\n",
        "                # https://pytorch.org/docs/stable/torchvision/models.html\n",
        "                torchvision.transforms.Normalize(\n",
        "                    mean=(0.485, 0.456, 0.406), \n",
        "                    std=(0.229, 0.224, 0.225)\n",
        "                ),\n",
        "            ]\n",
        "        )\n",
        "        return image_transform\n",
        "\n",
        "    def _build_dataset(self, data_key, img_dir):\n",
        "        return MemeDataset(\n",
        "            data_path = self.hparams.get(data_key),\n",
        "            img_dir = self.hparams.get(img_dir), \n",
        "            image_transform = self.image_transform,\n",
        "            text_transform = self.text_transform,\n",
        "            # limit training samples only\n",
        "        )\n",
        "\n",
        "    def _build_model(self):\n",
        "        # we're going to pass the outputs of our text\n",
        "        # transform through an additional trainable layer\n",
        "        # rather than fine-tuning the transform\n",
        "        language_module = torch.nn.Linear(\n",
        "                in_features=self.embedding_dim,\n",
        "                out_features=self.language_feature_dim\n",
        "        )\n",
        "        \n",
        "        # easiest way to get features rather than\n",
        "        # classification is to overwrite last layer\n",
        "        # with an identity transformation, we'll reduce\n",
        "        # dimension using a Linear layer, resnet is 2048 out\n",
        "        vision_module = torchvision.models.resnet152(\n",
        "            pretrained=True\n",
        "        )\n",
        "        vision_module.fc = torch.nn.Linear(\n",
        "                in_features=2048,\n",
        "                out_features=self.vision_feature_dim\n",
        "        )\n",
        "\n",
        "        return LangAndVisionConcat(\n",
        "            num_classes = self.hparams.get(\"num_classes\", 2),\n",
        "            loss_fn = torch.nn.CrossEntropyLoss(),\n",
        "            language_module = language_module,\n",
        "            vision_module = vision_module,\n",
        "            language_feature_dim = self.language_feature_dim,\n",
        "            vision_feature_dim = self.vision_feature_dim,\n",
        "            fusion_output_size = self.hparams.get(\n",
        "                \"fusion_output_size\", 512\n",
        "            ),\n",
        "            dropout_p = self.hparams.get(\"dropout_p\", 0.1),\n",
        "        )\n",
        "\n",
        "    def _get_trainer_params(self):\n",
        "        checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
        "            dirpath = self.output_path,\n",
        "            monitor = self.hparams.get(\n",
        "                \"checkpoint_monitor\", \"val_loss\"\n",
        "            ),\n",
        "            mode = self.hparams.get(\n",
        "                \"checkpoint_monitor_mode\", \"max\"\n",
        "            ),\n",
        "            verbose = self.hparams.get(\"verbose\", True)\n",
        "        )\n",
        "\n",
        "        early_stop_callback = pl.callbacks.EarlyStopping(\n",
        "            monitor = self.hparams.get(\n",
        "                \"early_stop_monitor\", \"val_loss\"\n",
        "            ),\n",
        "            min_delta = self.hparams.get(\n",
        "                \"early_stop_min_delta\", 0.001\n",
        "            ),\n",
        "            patience = self.hparams.get(\n",
        "                \"early_stop_patience\", 5\n",
        "            ),\n",
        "            verbose = self.hparams.get(\"verbose\", True),\n",
        "        )\n",
        "\n",
        "        trainer_params = {\n",
        "            \"callbacks\" : [early_stop_callback],\n",
        "            \"checkpoint_callback\": checkpoint_callback,\n",
        "            #\"early_stop_callback\": early_stop_callback,\n",
        "            #\"default_save_path\": self.output_path,\n",
        "            \"accumulate_grad_batches\": self.hparams.get(\n",
        "                \"accumulate_grad_batches\", 1\n",
        "            ),\n",
        "            \"gpus\": self.hparams.get(\"n_gpu\", 1),\n",
        "            \"max_epochs\": self.hparams.get(\"max_epochs\", 100),\n",
        "            \"gradient_clip_val\": self.hparams.get(\n",
        "                \"gradient_clip_value\", 1\n",
        "            ),\n",
        "        }\n",
        "        return trainer_params\n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def make_submission_frame(self, train_path):\n",
        "        train_dataset = self._build_dataset(train_path)\n",
        "        submission_frame = pd.DataFrame(\n",
        "            index = train_dataset.samples_frame.file_name,\n",
        "            columns=[\"proba\", \"misogynous\"]\n",
        "        )\n",
        "\n",
        "        train_dataloader = torch.utils.data.DataLoader(\n",
        "            train_dataset, \n",
        "            shuffle = False, \n",
        "            batch_size = self.hparams.get(\"batch_size\", 4), \n",
        "            num_workers = self.hparams.get(\"num_workers\", 16))\n",
        "        for batch in tqdm(train_dataloader, total = len(train_dataloader)):\n",
        "            preds, _ = self.model.eval().to(\"cpu\")(\n",
        "                batch[\"Text Transcription\"], batch[\"image\"]\n",
        "            )\n",
        "            submission_frame.loc[batch[\"file_name\"], \"proba\"] = preds[:, 1]\n",
        "            submission_frame.loc[batch[\"file_name\"], \"misogynous\"] = preds.argmax(dim=1)\n",
        "        submission_frame.proba = submission_frame.proba.astype(float)\n",
        "        submission_frame.label = submission_frame.misogynous.astype(int)\n",
        "        return submission_frame    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cuLS0aQWRS1"
      },
      "outputs": [],
      "source": [
        "hparams = {\n",
        "    \"train_path\": train_csv_path,\n",
        "    \"trial_path\": trial_csv_path,\n",
        "    \"train_img_dir\": data_dir,\n",
        "    \"trial_img_dir\": trial_path,\n",
        "\n",
        "    \n",
        "\n",
        "    \"embedding_dim\": 150,\n",
        "    \"language_feature_dim\": 300,\n",
        "    \"vision_feature_dim\": 300,\n",
        "    \"fusion_output_size\": 256,\n",
        "    \"output_path\": \"model-outputs\",\n",
        "    \"lr\": 0.005,\n",
        "    \"max_epochs\": 10,\n",
        "    \"n_gpu\": 1,\n",
        "    \"batch_size\": 8,\n",
        "    # allows us to \"simulate\" having larger batches \n",
        "    \"accumulate_grad_batches\": 16,\n",
        "    \"early_stop_patience\": 3,\n",
        "}\n",
        "\n",
        "\n",
        "miso_memes_model = MemesModel(hparams = hparams)\n",
        "miso_memes_model.fit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNYk4lPsYAQ8"
      },
      "outputs": [],
      "source": [
        "checkpoints = list(Path(\"model-outputs\").glob(\"*.ckpt\"))\n",
        "assert len(checkpoints) == 1\n",
        "\n",
        "checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "miso_memes_model = MemesModel.load_from_checkpoint(\n",
        "    checkpoints[0]\n",
        ")\n",
        "submission = miso_memes_model.make_submission_frame(\n",
        "    test_path\n",
        ")\n",
        "submission.head()"
      ],
      "metadata": {
        "id": "ESFhkcqRUoKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission.to_csv((\"out/submission.csv\"), index=True)"
      ],
      "metadata": {
        "id": "3jNfY06rUyqV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "AnotherMAMI.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}